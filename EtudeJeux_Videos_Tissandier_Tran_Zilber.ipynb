{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy.linalg as linalg\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List, Any \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_games = pd.read_csv(\"data/Jeux_Videos.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    genres = \"genres\" # qualitative nominal \n",
    "    overall_review = \"overall_review\" # qualitative ordinal\n",
    "    overall_review_percentage = \"overall_review_%\" # quantitative continue \n",
    "    awards = \"awards\" # quantitative discrete\n",
    "    categories = \"categories\"  # qualitative nominal\n",
    "    developer = \"developer\"  # qualitative nominal\n",
    "    publisher = \"publisher\"  # qualitative nominal\n",
    "    discounted_price = \"discounted_price\"  # quantitative continue\n",
    "    dlc_available = \"dlc_available\"  # quantitative discret\n",
    "    age_rating = \"age_rating\"  # qualitative nominal\n",
    "\n",
    "    original_price = \"original_price\"\n",
    "    discount_percentage = \"discount_percentage\"\n",
    "    \n",
    "    overall_review_order = [\n",
    "        \"Overwhelmingly Negative\", \"Very Negative\", \"Negative\", \"Mostly Negative\",\n",
    "        \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\", \"Overwhelmingly Positive\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Si besoin, on ordonne les notes dans le bon ordre\n",
    "overall_order = [\n",
    "    \"Overwhelmingly Negative\", \"Very Negative\", \"Negative\", \"Mostly Negative\",\n",
    "    \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\", \"Overwhelmingly Positive\"\n",
    "]\n",
    "\n",
    "# Charger les donn√©es (df = ton DataFrame d√©j√† nettoy√©)\n",
    "df = df[df['genres'].notnull() & df['overall_review'].notnull()]\n",
    "df['overall_review'] = pd.Categorical(df['overall_review'], categories=overall_order, ordered=True)\n",
    "\n",
    "# Extraire le genre principal si plusieurs genres sont list√©s\n",
    "df['main_genre'] = df['genres'].str.split(',').str[0]\n",
    "\n",
    "# Graphique en pourcentage d‚Äô√©valuations par genre principal\n",
    "genre_review_ct = pd.crosstab(df['main_genre'], df['overall_review'], normalize='index') * 100\n",
    "genre_review_ct = genre_review_ct[overall_order]  # garantir le bon ordre\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "genre_review_ct.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.title(\"R√©partition des √©valuations par genre principal\")\n",
    "plt.ylabel(\"Pourcentage des √©valuations (%)\")\n",
    "plt.xlabel(\"Genre principal\")\n",
    "plt.legend(title=\"√âvaluation globale\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ordre des reviews\n",
    "review_order = [\n",
    "    \"Overwhelmingly Negative\", \"Very Negative\", \"Negative\", \"Mostly Negative\",\n",
    "    \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\", \"Overwhelmingly Positive\"\n",
    "]\n",
    "\n",
    "# S'assurer que la variable est bien ordonn√©e\n",
    "df = df[df[\"overall_review\"].isin(review_order)].copy()\n",
    "df[\"overall_review\"] = pd.Categorical(df[\"overall_review\"], categories=review_order, ordered=True)\n",
    "\n",
    "# Nettoyage simple : enlever les prix n√©gatifs ou nuls\n",
    "df = df[df[\"discounted_price\"] > 0]\n",
    "\n",
    "# Statistiques descriptives\n",
    "group_stats = df.groupby(\"overall_review\")[\"discounted_price\"].describe()\n",
    "print(group_stats)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.boxplot(x=\"overall_review\", y=\"discounted_price\", data=df, order=review_order)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"√âvaluation globale du jeu\")\n",
    "plt.ylabel(\"Prix apr√®s r√©duction (‚Ç¨)\")\n",
    "plt.title(\"Prix apr√®s r√©duction en fonction des √©valuations des joueurs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement des donn√©es\n",
    "video_games = pd.read_csv(\"data/Jeux_Videos.csv\", sep=\",\")\n",
    "\n",
    "# Ordre pour la variable ordinale\n",
    "review_order = [\n",
    "    \"Overwhelmingly Negative\", \"Very Negative\", \"Negative\", \"Mostly Negative\",\n",
    "    \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\", \"Overwhelmingly Positive\"\n",
    "]\n",
    "\n",
    "# Nettoyage : suppression des valeurs manquantes\n",
    "df = video_games[[Variable.genres, Variable.overall_review]].dropna()\n",
    "\n",
    "# Conversion en cat√©gorie ordonn√©e\n",
    "df[Variable.overall_review] = pd.Categorical(\n",
    "    df[Variable.overall_review],\n",
    "    categories=Variable.overall_review_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Optionnel : ne garder qu‚Äôun seul genre (si plusieurs genres s√©par√©s par virgule)\n",
    "df[Variable.genres] = df[Variable.genres].str.split(\",\").str[0].str.strip()\n",
    "\n",
    "# Crosstab : pourcentages par ligne (par niveau de review)\n",
    "ct = pd.crosstab(df[Variable.overall_review], df[Variable.genres], normalize='index') * 100\n",
    "\n",
    "# Affichage du tableau crois√©\n",
    "display(ct.round(1))\n",
    "\n",
    "# Visualisation heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(ct, annot=False, cmap='YlGnBu', cbar_kws={'label': '%'})\n",
    "plt.title('R√©partition des genres selon la note globale (overall_review)')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Avis global')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Nettoyage des donn√©es\n",
    "df = video_games[[Variable.overall_review, Variable.original_price]].dropna()\n",
    "\n",
    "# Conversion de la variable ordinale\n",
    "df[Variable.overall_review] = pd.Categorical(\n",
    "    df[Variable.overall_review],\n",
    "    categories=Variable.overall_review_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=Variable.overall_review, y=Variable.original_price, data=df)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"R√©partition du prix original selon l'avis global\")\n",
    "plt.xlabel(\"Avis global\")\n",
    "plt.ylabel(\"Prix original (‚Ç¨)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_price(value):\n",
    "    if isinstance(value, str):\n",
    "        # Supprime tout sauf les chiffres et le point\n",
    "        value = re.sub(r\"[^\\d.]\", \"\", value)\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_percentage(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"%\", \"\").strip()\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Applique le nettoyage\n",
    "df = video_games[[Variable.original_price, Variable.discount_percentage]].copy()\n",
    "df[Variable.original_price] = df[Variable.original_price].apply(clean_price)\n",
    "df[Variable.discount_percentage] = df[Variable.discount_percentage].apply(clean_percentage)\n",
    "\n",
    "# Supprime les NaN\n",
    "df = df.dropna()\n",
    "\n",
    "# Filtre des valeurs aberrantes\n",
    "df = df[(df[Variable.original_price] > 0) & (df[Variable.original_price] < 500)]\n",
    "\n",
    "\n",
    "# V√©rification de la taille\n",
    "if df.empty:\n",
    "    print(\"‚ùå Aucune donn√©e valide apr√®s le nettoyage. V√©rifie les valeurs ou les filtres.\")\n",
    "else:\n",
    "    # Graphique de r√©gression\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(\n",
    "        x=Variable.original_price,\n",
    "        y=Variable.discount_percentage,\n",
    "        data=df,\n",
    "        scatter_kws={'alpha': 0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    plt.title(\"R√©gression lin√©aire : r√©duction (%) vs prix original\")\n",
    "    plt.xlabel(\"Prix original (‚Ç¨)\")\n",
    "    plt.ylabel(\"Pourcentage de r√©duction (%)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # R√©gression lin√©aire avec sklearn\n",
    "    X = df[[Variable.original_price]]\n",
    "    y = df[Variable.discount_percentage]\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    print(f\"üìà R√©gression lin√©aire : y = {model.coef_[0]:.2f}x + {model.intercept_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Charger les donn√©es\n",
    "df = video_games\n",
    "\n",
    "# √âtape 1 : Nettoyage des donn√©es\n",
    "# Convertir original_price et discounted_price en num√©rique\n",
    "def clean_price(price):\n",
    "    if pd.isna(price) or price == 'Free':\n",
    "        return 0.0\n",
    "    price = str(price).replace('‚Çπ', '').replace(',', '').strip()\n",
    "    try:\n",
    "        return float(price)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['original_price'] = df['original_price'].apply(clean_price)\n",
    "df['discounted_price'] = df['discounted_price'].apply(clean_price)\n",
    "\n",
    "# Convertir discount_percentage en num√©rique\n",
    "def clean_discount(discount):\n",
    "    if pd.isna(discount) or discount == '':\n",
    "        return 0.0\n",
    "    discount = str(discount).replace('%', '').strip()\n",
    "    try:\n",
    "        return float(discount)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "df['discount_percentage'] = df['discount_percentage'].apply(clean_discount)\n",
    "\n",
    "# Remplacer les NaN dans overall_review_% et overall_review_count\n",
    "df['overall_review_%'].fillna(df['overall_review_%'].median(), inplace=True)\n",
    "df['overall_review_count'].fillna(0, inplace=True)\n",
    "\n",
    "# Variables explicatives quantitatives\n",
    "quant_vars = ['original_price', 'discount_percentage', 'overall_review_count', \n",
    "              'awards', 'dlc_available']\n",
    "# Variable cible\n",
    "target = 'overall_review_%'\n",
    "\n",
    "# √âtape 2 : Gestion des valeurs manquantes pour les variables quantitatives\n",
    "for var in quant_vars:\n",
    "    df[var].fillna(df[var].median(), inplace=True)\n",
    "\n",
    "# √âtape 3 : R√©gression lin√©aire multiple\n",
    "X = df[quant_vars]\n",
    "y = df[target]\n",
    "\n",
    "# Standardisation des variables explicatives\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Mod√®le de r√©gression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_scaled, y)\n",
    "\n",
    "# Coefficients et intercept\n",
    "print(\"Coefficients de la r√©gression lin√©aire multiple :\")\n",
    "for var, coef in zip(quant_vars, reg.coef_):\n",
    "    print(f\"{var}: {coef:.4f}\")\n",
    "print(f\"Intercept: {reg.intercept_:.4f}\")\n",
    "\n",
    "# Score R¬≤\n",
    "print(f\"R¬≤: {reg.score(X_scaled, y):.4f}\")\n",
    "\n",
    "# √âtape 4 : Analyse en Composantes Principales (ACP)\n",
    "# S√©lection des variables quantitatives pour l'ACP\n",
    "acp_vars = quant_vars + [target]\n",
    "X_acp = df[acp_vars].dropna()\n",
    "\n",
    "# Standardisation\n",
    "X_acp_scaled = scaler.fit_transform(X_acp)\n",
    "\n",
    "# ACP\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_acp_scaled)\n",
    "\n",
    "# Variance expliqu√©e\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"\\nVariance expliqu√©e par composante :\")\n",
    "for i, var in enumerate(explained_variance_ratio):\n",
    "    print(f\"Composante {i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "# Visualisation de l'ACP\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "plt.xlabel(f'Composante 1 ({explained_variance_ratio[0]*100:.2f}%)')\n",
    "plt.ylabel(f'Composante 2 ({explained_variance_ratio[1]*100:.2f}%)')\n",
    "plt.title('Projection des jeux sur les deux premi√®res composantes principales')\n",
    "plt.grid(True)\n",
    "\n",
    "# Cercle des corr√©lations\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, var in enumerate(acp_vars):\n",
    "    plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], \n",
    "              color='r', alpha=0.5)\n",
    "    plt.text(pca.components_[0, i]*1.15, pca.components_[1, i]*1.15, \n",
    "             var, color='r', ha='center', va='center')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('Composante 1')\n",
    "plt.ylabel('Composante 2')\n",
    "plt.title('Cercle des corr√©lations (ACP)')\n",
    "plt.grid(True)\n",
    "circle = plt.Circle((0, 0), 1, color='b', fill=False)\n",
    "plt.gca().add_artist(circle)\n",
    "plt.show()\n",
    "\n",
    "# √âtape 5 : Analyse des corr√©lations\n",
    "corr_matrix = df[acp_vars].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Matrice de corr√©lation des variables quantitatives')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Cr√©er un document Word\n",
    "doc = Document()\n",
    "doc.add_heading('Tableau crois√© : Avis global vs Genre', level=1)\n",
    "\n",
    "# R√©cup√©rer le tableau crois√©\n",
    "table_data = ct.round(1).reset_index()  # remettre 'overall_review' en colonne\n",
    "\n",
    "# Cr√©er le tableau dans Word\n",
    "table = doc.add_table(rows=1, cols=len(table_data.columns))\n",
    "table.style = 'Table Grid'\n",
    "\n",
    "# Ajouter les en-t√™tes\n",
    "hdr_cells = table.rows[0].cells\n",
    "for i, col in enumerate(table_data.columns):\n",
    "    hdr_cells[i].text = str(col)\n",
    "\n",
    "# Ajouter les lignes\n",
    "for _, row in table_data.iterrows():\n",
    "    row_cells = table.add_row().cells\n",
    "    for i, item in enumerate(row):\n",
    "        row_cells[i].text = str(item)\n",
    "\n",
    "# Sauvegarder\n",
    "doc.save('tableau_croise.docx')\n",
    "print(\"Fichier Word g√©n√©r√© : tableau_croise.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_quantitative(series: pd.Series, cutted: bool = False, include_graph: bool = False) -> None:  \n",
    "    print(f\"\\n Taille de la population : {series.count()} observations\")  \n",
    "    print(f\" Minimum : {series.min()}\")  \n",
    "    print(f\" Maximum : {series.max()}\")  \n",
    "\n",
    "    if cutted:\n",
    "        # Calcul du pas\n",
    "        num_bins = math.floor(math.sqrt(series.count()))\n",
    "        \n",
    "        # D√©coupage en classes\n",
    "        cutted_series = pd.cut(series, bins=num_bins, precision=2)\n",
    "\n",
    "        # Effectif par classe\n",
    "        bin_counts = series.groupby(cutted_series).count()\n",
    "        bin_cumulative_counts = bin_counts.cumsum()\n",
    "        \n",
    "        # Fr√©quences\n",
    "        bin_frequencies = bin_counts / bin_counts.sum()\n",
    "        bin_cumulative_frequencies = bin_frequencies.cumsum()\n",
    "\n",
    "        # M√©dianes et modes\n",
    "        medians = series.groupby(cutted_series).median().reindex(bin_counts.index)\n",
    "        modes = series.groupby(cutted_series).apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).reindex(bin_counts.index)\n",
    "\n",
    "        # Cr√©ation du tableau avec alignement des longueurs\n",
    "        df_bins = pd.DataFrame({\n",
    "            \"Classe\": bin_counts.index.astype(str),\n",
    "            \"Effectif\": bin_counts.values,\n",
    "            \"Effectif cumul√©\": bin_cumulative_counts.values,\n",
    "            \"Fr√©quence\": bin_frequencies.values,\n",
    "            \"Fr√©quence cumul√©e\": bin_cumulative_frequencies.values,\n",
    "            \"M√©diane\": medians.values,\n",
    "            \"Mode\": modes.values\n",
    "        })\n",
    "\n",
    "        print(\"\\n Analyse par classes :\")\n",
    "        print(df_bins.to_string(index=False, formatters={\n",
    "            \"Fr√©quence\": \"{:.2%}\".format, \n",
    "            \"Fr√©quence cumul√©e\": \"{:.2%}\".format\n",
    "        }))\n",
    "\n",
    "    else:\n",
    "        # Analyse par valeur unique\n",
    "        counts = series.value_counts().sort_index()\n",
    "        cumulative_counts = counts.cumsum()\n",
    "        frequencies = counts / series.count()\n",
    "        cumulative_frequencies = frequencies.cumsum()\n",
    "\n",
    "        df_stats = pd.DataFrame({\n",
    "            \"Valeur\": counts.index,\n",
    "            \"Effectif\": counts.values,\n",
    "            \"Effectif cumul√©\": cumulative_counts.values,\n",
    "            \"Fr√©quence\": frequencies.values,\n",
    "            \"Fr√©quence cumul√©e\": cumulative_frequencies.values\n",
    "        })\n",
    "\n",
    "        print(\"\\n Analyse des valeurs uniques :\")\n",
    "        print(df_stats.to_string(index=False, formatters={\n",
    "            \"Fr√©quence\": \"{:.2%}\".format, \n",
    "            \"Fr√©quence cumul√©e\": \"{:.2%}\".format\n",
    "        }))\n",
    "\n",
    "        print(\"\\n Statistiques principales :\")\n",
    "        print(f\"- Moyenne : {series.mean():.3f}\")\n",
    "        print(f\"- M√©diane : {series.median():.3f}\")\n",
    "        print(f\"- Mode(s) : {', '.join(map(str, series.mode().tolist()))}\")\n",
    "        print(f\"- 25% (Q1) : {series.quantile(0.25):.3f}\")\n",
    "        print(f\"- 50% (Q2 - M√©diane) : {series.quantile(0.5):.3f}\")\n",
    "        print(f\"- 75% (Q3) : {series.quantile(0.75):.3f}\")\n",
    "        print(f\"- √âtendue : {series.max() - series.min():.3f}\")\n",
    "        print(f\"- √âcart interquartile (Q3 - Q1) : {series.quantile(0.75) - series.quantile(0.25):.3f}\")\n",
    "        print(f\"- √âcart-type : {series.std():.3f}\")\n",
    "        print(f\"- Variance : {series.var():.3f}\")\n",
    "\n",
    "    # Affichage des graphiques\n",
    "    if include_graph:\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        # Histogramme\n",
    "        ax[0].hist(series.dropna(), bins=math.floor(math.sqrt(series.count())), color=\"skyblue\", alpha=0.7, edgecolor=\"black\")\n",
    "        ax[0].set_xlabel(\"Valeurs\")\n",
    "        ax[0].set_ylabel(\"Effectif\")\n",
    "        ax[0].set_title(\"Histogramme des valeurs\")\n",
    "\n",
    "        # CDF (Courbe de r√©partition empirique)\n",
    "        sorted_data = np.sort(series.dropna())\n",
    "        cdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "        ax[1].plot(sorted_data, cdf, marker=\"o\", linestyle=\"-\", color=\"orange\")\n",
    "        ax[1].set_xlabel(\"Valeurs\")\n",
    "        ax[1].set_ylabel(\"Probabilit√© cumul√©e\")\n",
    "        ax[1].set_title(\"Fonction de r√©partition empirique (CDF)\")\n",
    "\n",
    "        # Boxplot\n",
    "        ax[2].boxplot(series.dropna(), vert=False, patch_artist=True, boxprops=dict(facecolor=\"lightblue\"))\n",
    "        ax[2].set_title(\"Boxplot de la variable\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_qualitative_ordinal(series: pd.Series, include_graph: bool = False, custom_order: List[Any] = []) -> None:\n",
    "    print(f\"\\n Taille de la population : {series.count()} observations\")  # Nombre total d'observations\n",
    "    \n",
    "    # V√©rifier si un ordre personnalis√© est fourni\n",
    "    if custom_order:\n",
    "        value_counts = series.value_counts()\n",
    "        counts = value_counts.reindex(custom_order).dropna()\n",
    "    else:\n",
    "        counts = series.value_counts().sort_index()  # Tri automatique si pas d'ordre personnalis√©\n",
    "\n",
    "    # Calcul des effectifs cumul√©s\n",
    "    cumulative_counts = counts.cumsum()\n",
    "\n",
    "    # Calcul des fr√©quences\n",
    "    frequencies = counts / series.count()\n",
    "    cumulative_frequencies = frequencies.cumsum()\n",
    "\n",
    "    # D√©termination du mode (valeur la plus fr√©quente)\n",
    "    max_effectif = counts.max()\n",
    "    mode_values = counts[counts == max_effectif].index.tolist()\n",
    "\n",
    "    # D√©termination de la m√©diane (valeur centrale)\n",
    "    cumulative_counts_half = cumulative_counts[cumulative_counts >= series.count() / 2]\n",
    "    mediane = cumulative_counts_half.index[0] if not cumulative_counts_half.empty else \"Ind√©termin√©e\"\n",
    "\n",
    "    # Cr√©ation du DataFrame pour affichage\n",
    "    df_stats = pd.DataFrame({\n",
    "        \"Valeur\": counts.index,\n",
    "        \"Effectif\": counts.values,\n",
    "        \"Effectif cumul√©\": cumulative_counts.values,\n",
    "        \"Fr√©quence\": frequencies.values,\n",
    "        \"Fr√©quence cumul√©e\": cumulative_frequencies.values\n",
    "    })\n",
    "\n",
    "    # Affichage du tableau format√©\n",
    "    print(\"\\n Analyse de la variable ordinale :\")\n",
    "    print(df_stats.to_string(index=False, formatters={\n",
    "        \"Fr√©quence\": \"{:.2%}\".format, \n",
    "        \"Fr√©quence cumul√©e\": \"{:.2%}\".format\n",
    "    }))\n",
    "\n",
    "    # Affichage des statistiques compl√©mentaires\n",
    "    print(\"\\n Statistiques compl√©mentaires :\")\n",
    "    print(f\"- Mode(s) : {', '.join(map(str, mode_values))}\")\n",
    "    print(f\"- M√©diane : {mediane}\")\n",
    "\n",
    "    # Affichage des graphiques si demand√©\n",
    "    if include_graph:\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Diagramme en barres (r√©partition des valeurs)\n",
    "        ax[0].bar(df_stats[\"Valeur\"], df_stats[\"Effectif\"], color=\"skyblue\", alpha=0.7)\n",
    "        ax[0].set_xlabel(\"Valeurs\")\n",
    "        ax[0].set_ylabel(\"Effectif\")\n",
    "        ax[0].set_title(\"R√©partition des valeurs ordinales\")\n",
    "        ax[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # Courbe des effectifs cumul√©s\n",
    "        ax[1].plot(df_stats[\"Valeur\"], df_stats[\"Effectif cumul√©\"], marker=\"o\", linestyle=\"-\", color=\"orange\")\n",
    "        ax[1].set_xlabel(\"Valeurs\")\n",
    "        ax[1].set_ylabel(\"Effectif cumul√©\")\n",
    "        ax[1].set_title(\"Effectifs cumul√©s\")\n",
    "\n",
    "        # Boxplot\n",
    "        if custom_order:\n",
    "            mapping = {val: i for i, val in enumerate(custom_order)}  # Associer chaque valeur √† un nombre\n",
    "            series_numeric = series.map(mapping).dropna()  # Appliquer la conversion\n",
    "            ax[2].boxplot(series_numeric, vert=False, patch_artist=True, boxprops=dict(facecolor=\"lightblue\"))\n",
    "            ax[2].set_title(\"Boxplot (Valeurs ordinales converties)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_qualitative_nominal(series: pd.Series, include_graph: bool = False) -> None:\n",
    "    # Initialiser un compteur pour stocker les fr√©quences des genres\n",
    "    compteur_genres = Counter()\n",
    "\n",
    "    # Parcourir les lignes et compter les occurrences des genres\n",
    "    for genres in series.dropna():\n",
    "        liste_genres = [genre.strip() for genre in genres.split(\",\")]\n",
    "        compteur_genres.update(liste_genres)\n",
    "\n",
    "    # Trier les genres par fr√©quence d√©croissante\n",
    "    genres_tries = dict(sorted(compteur_genres.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # Calcul des effectifs, effectifs cumul√©s, fr√©quences et fr√©quences cumul√©es\n",
    "    total = sum(genres_tries.values())\n",
    "    effectifs_cumules = []\n",
    "    frequences = []\n",
    "    frequences_cumulees = []\n",
    "\n",
    "    cumul = 0\n",
    "    for genre, effectif in genres_tries.items():\n",
    "        cumul += effectif\n",
    "        effectifs_cumules.append(cumul)\n",
    "        frequence = effectif / total\n",
    "        frequences.append(frequence)\n",
    "        frequences_cumulees.append(cumul / total)\n",
    "\n",
    "    # D√©termination du mode (les valeurs les plus fr√©quentes)\n",
    "    max_effectif = max(genres_tries.values())\n",
    "    mode_genres = [genre for genre, effectif in genres_tries.items() if effectif == max_effectif]\n",
    "\n",
    "    # Cr√©ation du DataFrame pour affichage\n",
    "    df_stats = pd.DataFrame({\n",
    "        \"Genre\": list(genres_tries.keys()),\n",
    "        \"Effectif\": list(genres_tries.values()),\n",
    "        \"Effectif cumul√©\": effectifs_cumules,\n",
    "        \"Fr√©quence\": frequences,\n",
    "        \"Fr√©quence cumul√©e\": frequences_cumulees\n",
    "    })\n",
    "\n",
    "    # Affichage du tableau des statistiques\n",
    "    print(\"\\nAnalyse des genres de jeux vid√©o :\")\n",
    "    print(df_stats.to_string(index=False, formatters={\"Fr√©quence\": \"{:.2%}\".format, \"Fr√©quence cumul√©e\": \"{:.2%}\".format}))\n",
    "    \n",
    "    # Affichage des statistiques suppl√©mentaires\n",
    "    print(\"\\nMode(s) des genres les plus fr√©quents :\", \", \".join(mode_genres))\n",
    "\n",
    "    # Affichage des graphiques si demand√©\n",
    "    if include_graph:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Histogramme (Bar Chart)\n",
    "        ax[0].bar(df_stats[\"Genre\"], df_stats[\"Effectif\"], color=\"skyblue\", alpha=0.7)\n",
    "        ax[0].set_xlabel(\"Genres\")\n",
    "        ax[0].set_ylabel(\"Effectif\")\n",
    "        ax[0].set_title(\"R√©partition des genres\")\n",
    "        ax[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # Diagramme en camembert (Pie Chart)\n",
    "        ax[1].pie(df_stats[\"Effectif\"], labels=df_stats[\"Genre\"], autopct=\"%1.1f%%\", colors=plt.cm.Paired.colors)\n",
    "        ax[1].set_title(\"R√©partition des genres (Pie Chart)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_qualitative_nominal(video_games[Variable.genres], include_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_quantitative(video_games[Variable.overall_review_percentage], cutted=False, include_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_qualitative_ordinal(video_games[Variable.overall_review], include_graph=True, custom_order=Variable.overall_review_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_quantitative(video_games[Variable.awards], cutted=False, include_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probl√©matiques bivari√©es\n",
    "- 1. Y a-t-il une relation entre le prix initial (original_price) et le pourcentage de r√©duction (discount_percentage) des jeux ?\n",
    "- 2. Le pourcentage de critiques positives (overall_review_%) varie-t-il selon la cat√©gorie du jeu (categories) ?\n",
    "- 3. Les jeux les mieux not√©s sont-ils vendus plus chers, m√™me apr√®s r√©duction ?\n",
    "- 4. Dans le contexte de la distribution num√©rique de jeux vid√©o, les r√©ductions appliqu√©es sur les titres sont un levier marketing majeur pour attirer les consommateurs. Ces remises sont particuli√®rement visibles lors d‚Äô√©v√©nements promotionnels majeurs comme les soldes Steam, les festivals de jeux ind√©pendants ou les ventes flash. Cependant, une question se pose : les jeux vid√©o vendus √† un prix plus √©lev√© b√©n√©ficient-ils d‚Äôun pourcentage de r√©duction plus important ?\n",
    "- 5. Comment les caract√©ristiques d‚Äôun jeu vid√©o, telles que le prix original, le pourcentage de r√©duction, le nombre de critiques, les r√©compenses et la disponibilit√© de DLC, influencent-elles le pourcentage global des critiques positives (overall_review_%) sur une plateforme comme Steam ? L‚Äôobjectif est d‚Äôidentifier les facteurs cl√©s qui contribuent √† la r√©ception critique d‚Äôun jeu.\n",
    "\n",
    "## R√©ponses partielles\n",
    "- 1. Une analyse pr√©liminaire pourrait consister √† calculer le coefficient de corr√©lation de Pearson entre ces deux variables quantitatives pour √©valuer la force et la direction de leur relation (positive, n√©gative ou nulle). On pourrait aussi visualiser cette relation avec un nuage de points (scatter plot).\n",
    "- 2. On pourrait comparer la moyenne du pourcentage de critiques positives pour chaque cat√©gorie (ex. Solo, Multiplayer) √† l‚Äôaide d‚Äôun test statistique comme une ANOVA ou un test de Kruskal-Wallis si les donn√©es ne sont pas normales. Un boxplot pourrait illustrer les diff√©rences.\n",
    "- 3. L‚Äôintuition sous-jacente est que la qualit√© per√ßue d‚Äôun jeu influe sur sa capacit√© √† conserver un prix √©lev√© malgr√© les remises. Un √©diteur peut se permettre de moins baisser le prix d‚Äôun jeu pl√©biscit√© par les joueurs.\n",
    "- 4. Cette interrogation soul√®ve une probl√©matique plus large, qui concerne la strat√©gie commerciale des √©diteurs. Ces derniers peuvent, en th√©orie, appliquer des r√©ductions plus importantes sur les jeux chers afin de compenser un prix de d√©part dissuasif. √Ä l‚Äôinverse, ils pourraient maintenir une r√©duction faible pour ne pas d√©valuer la valeur per√ßue du produit. \n",
    "- 5. Nous supposons que des variables comme un prix original √©lev√©, un pourcentage de r√©duction important, un grand nombre de critiques, des r√©compenses et la pr√©sence de DLC ont un impact significatif sur le pourcentage de critiques positives. Plus pr√©cis√©ment, nous pensons que les jeux avec plus de critiques et de r√©compenses auront un meilleur pourcentage de critiques positives, tandis qu‚Äôun prix √©lev√© pourrait avoir un effet n√©gatif.\n",
    "## Probl√©matiques gard√©es\n",
    "- 3. Les jeux les mieux not√©s sont-ils vendus plus chers, m√™me apr√®s r√©duction ?\n",
    "- 4. Dans le contexte de la distribution num√©rique de jeux vid√©o, les r√©ductions appliqu√©es sur les titres sont un levier marketing majeur pour attirer les consommateurs. Ces remises sont particuli√®rement visibles lors d‚Äô√©v√©nements promotionnels majeurs comme les soldes Steam, les festivals de jeux ind√©pendants ou les ventes flash. Cependant, une question se pose : les jeux vid√©o vendus √† un prix plus √©lev√© b√©n√©ficient-ils d‚Äôun pourcentage de r√©duction plus important ?\n",
    "- 5. Comment les caract√©ristiques d‚Äôun jeu vid√©o, telles que le prix original, le pourcentage de r√©duction, le nombre de critiques, les r√©compenses et la disponibilit√© de DLC, influencent-elles le pourcentage global des critiques positives (overall_review_%) sur une plateforme comme Steam ? L‚Äôobjectif est d‚Äôidentifier les facteurs cl√©s qui contribuent √† la r√©ception critique d‚Äôun jeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des prix et pourcentages en valeurs num√©riques\n",
    "def clean_price(price_str):\n",
    "    if pd.isna(price_str) or price_str == 'Free' or price_str == '':\n",
    "        return 0\n",
    "    # Extraction du prix num√©rique (sans devise)\n",
    "    return float(''.join(char for char in price_str if char.isdigit() or char == '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_discount(discount_str):\n",
    "    if pd.isna(discount_str) or discount_str == '':\n",
    "        return 0\n",
    "    # Extraction du pourcentage (sans le signe %)\n",
    "    return float(discount_str.strip('%').strip('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_problem_1(original_price_series: pd.Series, discount_percentage_series: pd.Series, include_graph: bool = False) -> None:\n",
    "        # Chargement des donn√©es\n",
    "    # La premi√®re ligne est d√©j√† d√©finie:\n",
    "    # video_games = pd.read_csv(\"data/Jeux_Videos.csv\", sep=\",\")\n",
    "\n",
    "    clean_price_series = original_price_series.apply(clean_price)\n",
    "    clean_discount_series = discount_percentage_series.apply(clean_discount)\n",
    "    \n",
    "    # Cr√©ation d'un DataFrame temporaire pour l'analyse\n",
    "    temp_df = pd.DataFrame({\n",
    "        'original_price_clean': clean_price_series,\n",
    "        'discount_percentage_clean': clean_discount_series\n",
    "    })\n",
    "    \n",
    "    # Filtrage des jeux payants avec des donn√©es valides\n",
    "    analysis_df = temp_df[temp_df['original_price_clean'] > 0].copy()\n",
    "    \n",
    "    print(\"============ ANALYSE DE LA RELATION PRIX INITIAL - R√âDUCTION ============\")\n",
    "    \n",
    "    # 1. Statistiques descriptives\n",
    "    print(\"\\n1. Statistiques descriptives:\")\n",
    "    print(analysis_df.describe())\n",
    "    \n",
    "    # V√©rification si les donn√©es sont suffisantes pour l'analyse\n",
    "    if len(analysis_df) < 2:\n",
    "        print(\"Pas assez de donn√©es pour effectuer l'analyse de corr√©lation.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Analyse de corr√©lation\n",
    "    correlation = analysis_df['original_price_clean'].corr(analysis_df['discount_percentage_clean'])\n",
    "    print(f\"\\n2. Coefficient de corr√©lation entre prix initial et r√©duction: {correlation:.4f}\")\n",
    "    \n",
    "    # Test de signification statistique (p-value)\n",
    "    corr_test = stats.pearsonr(analysis_df['original_price_clean'], analysis_df['discount_percentage_clean'])\n",
    "    print(f\"   p-value: {corr_test[1]:.4f}\")\n",
    "    \n",
    "    # Interpr√©tation du coefficient de corr√©lation\n",
    "    print(\"\\n   Interpr√©tation du coefficient de corr√©lation:\")\n",
    "    if abs(correlation) < 0.1:\n",
    "        strength = \"tr√®s faible voire inexistante\"\n",
    "    elif abs(correlation) < 0.3:\n",
    "        strength = \"faible\"\n",
    "    elif abs(correlation) < 0.5:\n",
    "        strength = \"mod√©r√©e\"\n",
    "    elif abs(correlation) < 0.7:\n",
    "        strength = \"forte\"\n",
    "    else:\n",
    "        strength = \"tr√®s forte\"\n",
    "        \n",
    "    direction = \"positive\" if correlation > 0 else \"n√©gative\"\n",
    "    significance = \"statistiquement significative\" if corr_test[1] < 0.05 else \"non statistiquement significative\"\n",
    "    \n",
    "    print(f\"   La corr√©lation est {strength} et {direction} ({significance}).\")\n",
    "    \n",
    "    # 3. Mod√©lisation lin√©aire\n",
    "    X = analysis_df[['original_price_clean']]\n",
    "    y = analysis_df['discount_percentage_clean']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    print(f\"\\n3. Mod√®le lin√©aire:\")\n",
    "    print(f\"   √âquation: discount_percentage = {model.coef_[0]:.4f} √ó original_price + {model.intercept_:.4f}\")\n",
    "    print(f\"   R¬≤ (coefficient de d√©termination): {model.score(X, y):.4f}\")\n",
    "    \n",
    "    # Interpr√©tation du mod√®le\n",
    "    print(\"\\n   Interpr√©tation du mod√®le:\")\n",
    "    if model.coef_[0] > 0:\n",
    "        print(f\"   Pour chaque augmentation de 1 unit√© du prix initial, le pourcentage de r√©duction augmente en moyenne de {model.coef_[0]:.4f} points.\")\n",
    "    else:\n",
    "        print(f\"   Pour chaque augmentation de 1 unit√© du prix initial, le pourcentage de r√©duction diminue en moyenne de {abs(model.coef_[0]):.4f} points.\")\n",
    "    \n",
    "    # 4. Analyse par tranches de prix\n",
    "    analysis_df['price_range'] = pd.cut(analysis_df['original_price_clean'], \n",
    "                                        bins=[0, 10, 20, 30, 40, 50, float('inf')],\n",
    "                                        labels=['0-10', '10-20', '20-30', '30-40', '40-50', '50+'])\n",
    "    \n",
    "    price_range_analysis = analysis_df.groupby('price_range')['discount_percentage_clean'].agg(['mean', 'count'])\n",
    "    print(\"\\n4. R√©duction moyenne par tranche de prix:\")\n",
    "    print(price_range_analysis)\n",
    "    \n",
    "    # 5. Visualisations (conditionnelles)\n",
    "    if include_graph:\n",
    "        # Nuage de points avec ligne de tendance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='original_price_clean', y='discount_percentage_clean', data=analysis_df, alpha=0.6)\n",
    "        sns.regplot(x='original_price_clean', y='discount_percentage_clean', data=analysis_df, \n",
    "                    scatter=False, color='red', line_kws={\"linestyle\": \"--\"})\n",
    "        plt.title('Relation entre prix initial et pourcentage de r√©duction')\n",
    "        plt.xlabel('Prix initial')\n",
    "        plt.ylabel('Pourcentage de r√©duction')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Graphique √† barres par tranche de prix\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=price_range_analysis.index, y='mean', data=price_range_analysis.reset_index())\n",
    "        plt.title('Pourcentage de r√©duction moyen par tranche de prix')\n",
    "        plt.xlabel('Tranche de prix')\n",
    "        plt.ylabel('R√©duction moyenne (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "        # 6. Conclusion\n",
    "        print(\"\\n5. Conclusion:\")\n",
    "        if abs(correlation) < 0.1:\n",
    "            print(\"   Il n'existe pas de relation lin√©aire claire entre le prix initial et le pourcentage de r√©duction.\")\n",
    "        else:\n",
    "            print(f\"   Il existe une relation {strength} {direction} entre le prix initial et le pourcentage de r√©duction.\")\n",
    "            \n",
    "        if model.score(X, y) < 0.3: \n",
    "            print(\"   Le mod√®le lin√©aire explique tr√®s peu la variance des r√©ductions, sugg√©rant que d'autres facteurs influencent davantage les politiques de r√©duction.\")\n",
    "        \n",
    "        print(\"\\n============================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_problem_1(Variable.original_price, Variable.discount_percentage, include_graph = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
