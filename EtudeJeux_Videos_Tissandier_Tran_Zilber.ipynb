{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy.linalg as linalg\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List, Any \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_games = pd.read_csv(\"data/Jeux_Videos.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    genres = \"genres\" # qualitative nominal \n",
    "    overall_review = \"overall_review\" # qualitative ordinal\n",
    "    overall_review_percentage = \"overall_review_%\" # quantitative continue \n",
    "    awards = \"awards\" # quantitative discrete\n",
    "    categories = \"categories\"  # qualitative nominal\n",
    "    developer = \"developer\"  # qualitative nominal\n",
    "    publisher = \"publisher\"  # qualitative nominal\n",
    "    discounted_price = \"discounted_price\"  # quantitative continue\n",
    "    dlc_available = \"dlc_available\"  # quantitative discret\n",
    "    age_rating = \"age_rating\"  # qualitative nominal\n",
    "\n",
    "    original_price = \"original_price\"\n",
    "    discount_percentage = \"discount_percentage\"\n",
    "    \n",
    "    overall_review_order = [\n",
    "        \"Overwhelmingly Negative\", \"Very Negative\", \"Negative\", \"Mostly Negative\",\n",
    "        \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\", \"Overwhelmingly Positive\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Si besoin, on ordonne les notes dans le bon ordre\n",
    "overall_order = [\n",
    "    \"Overwhelmingly Negative\", \"Very Negative\", \"Negative\", \"Mostly Negative\",\n",
    "    \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\", \"Overwhelmingly Positive\"\n",
    "]\n",
    "\n",
    "# Charger les données (df = ton DataFrame déjà nettoyé)\n",
    "df = df[df['genres'].notnull() & df['overall_review'].notnull()]\n",
    "df['overall_review'] = pd.Categorical(df['overall_review'], categories=overall_order, ordered=True)\n",
    "\n",
    "# Extraire le genre principal si plusieurs genres sont listés\n",
    "df['main_genre'] = df['genres'].str.split(',').str[0]\n",
    "\n",
    "# Graphique en pourcentage d’évaluations par genre principal\n",
    "genre_review_ct = pd.crosstab(df['main_genre'], df['overall_review'], normalize='index') * 100\n",
    "genre_review_ct = genre_review_ct[overall_order]  # garantir le bon ordre\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "genre_review_ct.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.title(\"Répartition des évaluations par genre principal\")\n",
    "plt.ylabel(\"Pourcentage des évaluations (%)\")\n",
    "plt.xlabel(\"Genre principal\")\n",
    "plt.legend(title=\"Évaluation globale\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ordre des reviews\n",
    "review_order = [\n",
    "    \"Overwhelmingly Negative\", \"Very Negative\", \"Negative\", \"Mostly Negative\",\n",
    "    \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\", \"Overwhelmingly Positive\"\n",
    "]\n",
    "\n",
    "# S'assurer que la variable est bien ordonnée\n",
    "df = df[df[\"overall_review\"].isin(review_order)].copy()\n",
    "df[\"overall_review\"] = pd.Categorical(df[\"overall_review\"], categories=review_order, ordered=True)\n",
    "\n",
    "# Nettoyage simple : enlever les prix négatifs ou nuls\n",
    "df = df[df[\"discounted_price\"] > 0]\n",
    "\n",
    "# Statistiques descriptives\n",
    "group_stats = df.groupby(\"overall_review\")[\"discounted_price\"].describe()\n",
    "print(group_stats)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.boxplot(x=\"overall_review\", y=\"discounted_price\", data=df, order=review_order)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Évaluation globale du jeu\")\n",
    "plt.ylabel(\"Prix après réduction (€)\")\n",
    "plt.title(\"Prix après réduction en fonction des évaluations des joueurs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement des données\n",
    "video_games = pd.read_csv(\"data/Jeux_Videos.csv\", sep=\",\")\n",
    "\n",
    "# Ordre pour la variable ordinale\n",
    "review_order = [\n",
    "    \"Overwhelmingly Negative\", \"Very Negative\", \"Negative\", \"Mostly Negative\",\n",
    "    \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\", \"Overwhelmingly Positive\"\n",
    "]\n",
    "\n",
    "# Nettoyage : suppression des valeurs manquantes\n",
    "df = video_games[[Variable.genres, Variable.overall_review]].dropna()\n",
    "\n",
    "# Conversion en catégorie ordonnée\n",
    "df[Variable.overall_review] = pd.Categorical(\n",
    "    df[Variable.overall_review],\n",
    "    categories=Variable.overall_review_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Optionnel : ne garder qu’un seul genre (si plusieurs genres séparés par virgule)\n",
    "df[Variable.genres] = df[Variable.genres].str.split(\",\").str[0].str.strip()\n",
    "\n",
    "# Crosstab : pourcentages par ligne (par niveau de review)\n",
    "ct = pd.crosstab(df[Variable.overall_review], df[Variable.genres], normalize='index') * 100\n",
    "\n",
    "# Affichage du tableau croisé\n",
    "display(ct.round(1))\n",
    "\n",
    "# Visualisation heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(ct, annot=False, cmap='YlGnBu', cbar_kws={'label': '%'})\n",
    "plt.title('Répartition des genres selon la note globale (overall_review)')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Avis global')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Nettoyage des données\n",
    "df = video_games[[Variable.overall_review, Variable.original_price]].dropna()\n",
    "\n",
    "# Conversion de la variable ordinale\n",
    "df[Variable.overall_review] = pd.Categorical(\n",
    "    df[Variable.overall_review],\n",
    "    categories=Variable.overall_review_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=Variable.overall_review, y=Variable.original_price, data=df)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Répartition du prix original selon l'avis global\")\n",
    "plt.xlabel(\"Avis global\")\n",
    "plt.ylabel(\"Prix original (€)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_price(value):\n",
    "    if isinstance(value, str):\n",
    "        # Supprime tout sauf les chiffres et le point\n",
    "        value = re.sub(r\"[^\\d.]\", \"\", value)\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_percentage(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace(\"%\", \"\").strip()\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Applique le nettoyage\n",
    "df = video_games[[Variable.original_price, Variable.discount_percentage]].copy()\n",
    "df[Variable.original_price] = df[Variable.original_price].apply(clean_price)\n",
    "df[Variable.discount_percentage] = df[Variable.discount_percentage].apply(clean_percentage)\n",
    "\n",
    "# Supprime les NaN\n",
    "df = df.dropna()\n",
    "\n",
    "# Filtre des valeurs aberrantes\n",
    "df = df[(df[Variable.original_price] > 0) & (df[Variable.original_price] < 500)]\n",
    "\n",
    "\n",
    "# Vérification de la taille\n",
    "if df.empty:\n",
    "    print(\"❌ Aucune donnée valide après le nettoyage. Vérifie les valeurs ou les filtres.\")\n",
    "else:\n",
    "    # Graphique de régression\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(\n",
    "        x=Variable.original_price,\n",
    "        y=Variable.discount_percentage,\n",
    "        data=df,\n",
    "        scatter_kws={'alpha': 0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    plt.title(\"Régression linéaire : réduction (%) vs prix original\")\n",
    "    plt.xlabel(\"Prix original (€)\")\n",
    "    plt.ylabel(\"Pourcentage de réduction (%)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Régression linéaire avec sklearn\n",
    "    X = df[[Variable.original_price]]\n",
    "    y = df[Variable.discount_percentage]\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    print(f\"📈 Régression linéaire : y = {model.coef_[0]:.2f}x + {model.intercept_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Charger les données\n",
    "df = video_games\n",
    "\n",
    "# Étape 1 : Nettoyage des données\n",
    "# Convertir original_price et discounted_price en numérique\n",
    "def clean_price(price):\n",
    "    if pd.isna(price) or price == 'Free':\n",
    "        return 0.0\n",
    "    price = str(price).replace('₹', '').replace(',', '').strip()\n",
    "    try:\n",
    "        return float(price)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['original_price'] = df['original_price'].apply(clean_price)\n",
    "df['discounted_price'] = df['discounted_price'].apply(clean_price)\n",
    "\n",
    "# Convertir discount_percentage en numérique\n",
    "def clean_discount(discount):\n",
    "    if pd.isna(discount) or discount == '':\n",
    "        return 0.0\n",
    "    discount = str(discount).replace('%', '').strip()\n",
    "    try:\n",
    "        return float(discount)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "df['discount_percentage'] = df['discount_percentage'].apply(clean_discount)\n",
    "\n",
    "# Remplacer les NaN dans overall_review_% et overall_review_count\n",
    "df['overall_review_%'].fillna(df['overall_review_%'].median(), inplace=True)\n",
    "df['overall_review_count'].fillna(0, inplace=True)\n",
    "\n",
    "# Variables explicatives quantitatives\n",
    "quant_vars = ['original_price', 'discount_percentage', 'overall_review_count', \n",
    "              'awards', 'dlc_available']\n",
    "# Variable cible\n",
    "target = 'overall_review_%'\n",
    "\n",
    "# Étape 2 : Gestion des valeurs manquantes pour les variables quantitatives\n",
    "for var in quant_vars:\n",
    "    df[var].fillna(df[var].median(), inplace=True)\n",
    "\n",
    "# Étape 3 : Régression linéaire multiple\n",
    "X = df[quant_vars]\n",
    "y = df[target]\n",
    "\n",
    "# Standardisation des variables explicatives\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Modèle de régression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_scaled, y)\n",
    "\n",
    "# Coefficients et intercept\n",
    "print(\"Coefficients de la régression linéaire multiple :\")\n",
    "for var, coef in zip(quant_vars, reg.coef_):\n",
    "    print(f\"{var}: {coef:.4f}\")\n",
    "print(f\"Intercept: {reg.intercept_:.4f}\")\n",
    "\n",
    "# Score R²\n",
    "print(f\"R²: {reg.score(X_scaled, y):.4f}\")\n",
    "\n",
    "# Étape 4 : Analyse en Composantes Principales (ACP)\n",
    "# Sélection des variables quantitatives pour l'ACP\n",
    "acp_vars = quant_vars + [target]\n",
    "X_acp = df[acp_vars].dropna()\n",
    "\n",
    "# Standardisation\n",
    "X_acp_scaled = scaler.fit_transform(X_acp)\n",
    "\n",
    "# ACP\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_acp_scaled)\n",
    "\n",
    "# Variance expliquée\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"\\nVariance expliquée par composante :\")\n",
    "for i, var in enumerate(explained_variance_ratio):\n",
    "    print(f\"Composante {i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "# Visualisation de l'ACP\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "plt.xlabel(f'Composante 1 ({explained_variance_ratio[0]*100:.2f}%)')\n",
    "plt.ylabel(f'Composante 2 ({explained_variance_ratio[1]*100:.2f}%)')\n",
    "plt.title('Projection des jeux sur les deux premières composantes principales')\n",
    "plt.grid(True)\n",
    "\n",
    "# Cercle des corrélations\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, var in enumerate(acp_vars):\n",
    "    plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], \n",
    "              color='r', alpha=0.5)\n",
    "    plt.text(pca.components_[0, i]*1.15, pca.components_[1, i]*1.15, \n",
    "             var, color='r', ha='center', va='center')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('Composante 1')\n",
    "plt.ylabel('Composante 2')\n",
    "plt.title('Cercle des corrélations (ACP)')\n",
    "plt.grid(True)\n",
    "circle = plt.Circle((0, 0), 1, color='b', fill=False)\n",
    "plt.gca().add_artist(circle)\n",
    "plt.show()\n",
    "\n",
    "# Étape 5 : Analyse des corrélations\n",
    "corr_matrix = df[acp_vars].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Matrice de corrélation des variables quantitatives')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Créer un document Word\n",
    "doc = Document()\n",
    "doc.add_heading('Tableau croisé : Avis global vs Genre', level=1)\n",
    "\n",
    "# Récupérer le tableau croisé\n",
    "table_data = ct.round(1).reset_index()  # remettre 'overall_review' en colonne\n",
    "\n",
    "# Créer le tableau dans Word\n",
    "table = doc.add_table(rows=1, cols=len(table_data.columns))\n",
    "table.style = 'Table Grid'\n",
    "\n",
    "# Ajouter les en-têtes\n",
    "hdr_cells = table.rows[0].cells\n",
    "for i, col in enumerate(table_data.columns):\n",
    "    hdr_cells[i].text = str(col)\n",
    "\n",
    "# Ajouter les lignes\n",
    "for _, row in table_data.iterrows():\n",
    "    row_cells = table.add_row().cells\n",
    "    for i, item in enumerate(row):\n",
    "        row_cells[i].text = str(item)\n",
    "\n",
    "# Sauvegarder\n",
    "doc.save('tableau_croise.docx')\n",
    "print(\"Fichier Word généré : tableau_croise.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_quantitative(series: pd.Series, cutted: bool = False, include_graph: bool = False) -> None:  \n",
    "    print(f\"\\n Taille de la population : {series.count()} observations\")  \n",
    "    print(f\" Minimum : {series.min()}\")  \n",
    "    print(f\" Maximum : {series.max()}\")  \n",
    "\n",
    "    if cutted:\n",
    "        # Calcul du pas\n",
    "        num_bins = math.floor(math.sqrt(series.count()))\n",
    "        \n",
    "        # Découpage en classes\n",
    "        cutted_series = pd.cut(series, bins=num_bins, precision=2)\n",
    "\n",
    "        # Effectif par classe\n",
    "        bin_counts = series.groupby(cutted_series).count()\n",
    "        bin_cumulative_counts = bin_counts.cumsum()\n",
    "        \n",
    "        # Fréquences\n",
    "        bin_frequencies = bin_counts / bin_counts.sum()\n",
    "        bin_cumulative_frequencies = bin_frequencies.cumsum()\n",
    "\n",
    "        # Médianes et modes\n",
    "        medians = series.groupby(cutted_series).median().reindex(bin_counts.index)\n",
    "        modes = series.groupby(cutted_series).apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).reindex(bin_counts.index)\n",
    "\n",
    "        # Création du tableau avec alignement des longueurs\n",
    "        df_bins = pd.DataFrame({\n",
    "            \"Classe\": bin_counts.index.astype(str),\n",
    "            \"Effectif\": bin_counts.values,\n",
    "            \"Effectif cumulé\": bin_cumulative_counts.values,\n",
    "            \"Fréquence\": bin_frequencies.values,\n",
    "            \"Fréquence cumulée\": bin_cumulative_frequencies.values,\n",
    "            \"Médiane\": medians.values,\n",
    "            \"Mode\": modes.values\n",
    "        })\n",
    "\n",
    "        print(\"\\n Analyse par classes :\")\n",
    "        print(df_bins.to_string(index=False, formatters={\n",
    "            \"Fréquence\": \"{:.2%}\".format, \n",
    "            \"Fréquence cumulée\": \"{:.2%}\".format\n",
    "        }))\n",
    "\n",
    "    else:\n",
    "        # Analyse par valeur unique\n",
    "        counts = series.value_counts().sort_index()\n",
    "        cumulative_counts = counts.cumsum()\n",
    "        frequencies = counts / series.count()\n",
    "        cumulative_frequencies = frequencies.cumsum()\n",
    "\n",
    "        df_stats = pd.DataFrame({\n",
    "            \"Valeur\": counts.index,\n",
    "            \"Effectif\": counts.values,\n",
    "            \"Effectif cumulé\": cumulative_counts.values,\n",
    "            \"Fréquence\": frequencies.values,\n",
    "            \"Fréquence cumulée\": cumulative_frequencies.values\n",
    "        })\n",
    "\n",
    "        print(\"\\n Analyse des valeurs uniques :\")\n",
    "        print(df_stats.to_string(index=False, formatters={\n",
    "            \"Fréquence\": \"{:.2%}\".format, \n",
    "            \"Fréquence cumulée\": \"{:.2%}\".format\n",
    "        }))\n",
    "\n",
    "        print(\"\\n Statistiques principales :\")\n",
    "        print(f\"- Moyenne : {series.mean():.3f}\")\n",
    "        print(f\"- Médiane : {series.median():.3f}\")\n",
    "        print(f\"- Mode(s) : {', '.join(map(str, series.mode().tolist()))}\")\n",
    "        print(f\"- 25% (Q1) : {series.quantile(0.25):.3f}\")\n",
    "        print(f\"- 50% (Q2 - Médiane) : {series.quantile(0.5):.3f}\")\n",
    "        print(f\"- 75% (Q3) : {series.quantile(0.75):.3f}\")\n",
    "        print(f\"- Étendue : {series.max() - series.min():.3f}\")\n",
    "        print(f\"- Écart interquartile (Q3 - Q1) : {series.quantile(0.75) - series.quantile(0.25):.3f}\")\n",
    "        print(f\"- Écart-type : {series.std():.3f}\")\n",
    "        print(f\"- Variance : {series.var():.3f}\")\n",
    "\n",
    "    # Affichage des graphiques\n",
    "    if include_graph:\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        # Histogramme\n",
    "        ax[0].hist(series.dropna(), bins=math.floor(math.sqrt(series.count())), color=\"skyblue\", alpha=0.7, edgecolor=\"black\")\n",
    "        ax[0].set_xlabel(\"Valeurs\")\n",
    "        ax[0].set_ylabel(\"Effectif\")\n",
    "        ax[0].set_title(\"Histogramme des valeurs\")\n",
    "\n",
    "        # CDF (Courbe de répartition empirique)\n",
    "        sorted_data = np.sort(series.dropna())\n",
    "        cdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "        ax[1].plot(sorted_data, cdf, marker=\"o\", linestyle=\"-\", color=\"orange\")\n",
    "        ax[1].set_xlabel(\"Valeurs\")\n",
    "        ax[1].set_ylabel(\"Probabilité cumulée\")\n",
    "        ax[1].set_title(\"Fonction de répartition empirique (CDF)\")\n",
    "\n",
    "        # Boxplot\n",
    "        ax[2].boxplot(series.dropna(), vert=False, patch_artist=True, boxprops=dict(facecolor=\"lightblue\"))\n",
    "        ax[2].set_title(\"Boxplot de la variable\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_qualitative_ordinal(series: pd.Series, include_graph: bool = False, custom_order: List[Any] = []) -> None:\n",
    "    print(f\"\\n Taille de la population : {series.count()} observations\")  # Nombre total d'observations\n",
    "    \n",
    "    # Vérifier si un ordre personnalisé est fourni\n",
    "    if custom_order:\n",
    "        value_counts = series.value_counts()\n",
    "        counts = value_counts.reindex(custom_order).dropna()\n",
    "    else:\n",
    "        counts = series.value_counts().sort_index()  # Tri automatique si pas d'ordre personnalisé\n",
    "\n",
    "    # Calcul des effectifs cumulés\n",
    "    cumulative_counts = counts.cumsum()\n",
    "\n",
    "    # Calcul des fréquences\n",
    "    frequencies = counts / series.count()\n",
    "    cumulative_frequencies = frequencies.cumsum()\n",
    "\n",
    "    # Détermination du mode (valeur la plus fréquente)\n",
    "    max_effectif = counts.max()\n",
    "    mode_values = counts[counts == max_effectif].index.tolist()\n",
    "\n",
    "    # Détermination de la médiane (valeur centrale)\n",
    "    cumulative_counts_half = cumulative_counts[cumulative_counts >= series.count() / 2]\n",
    "    mediane = cumulative_counts_half.index[0] if not cumulative_counts_half.empty else \"Indéterminée\"\n",
    "\n",
    "    # Création du DataFrame pour affichage\n",
    "    df_stats = pd.DataFrame({\n",
    "        \"Valeur\": counts.index,\n",
    "        \"Effectif\": counts.values,\n",
    "        \"Effectif cumulé\": cumulative_counts.values,\n",
    "        \"Fréquence\": frequencies.values,\n",
    "        \"Fréquence cumulée\": cumulative_frequencies.values\n",
    "    })\n",
    "\n",
    "    # Affichage du tableau formaté\n",
    "    print(\"\\n Analyse de la variable ordinale :\")\n",
    "    print(df_stats.to_string(index=False, formatters={\n",
    "        \"Fréquence\": \"{:.2%}\".format, \n",
    "        \"Fréquence cumulée\": \"{:.2%}\".format\n",
    "    }))\n",
    "\n",
    "    # Affichage des statistiques complémentaires\n",
    "    print(\"\\n Statistiques complémentaires :\")\n",
    "    print(f\"- Mode(s) : {', '.join(map(str, mode_values))}\")\n",
    "    print(f\"- Médiane : {mediane}\")\n",
    "\n",
    "    # Affichage des graphiques si demandé\n",
    "    if include_graph:\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Diagramme en barres (répartition des valeurs)\n",
    "        ax[0].bar(df_stats[\"Valeur\"], df_stats[\"Effectif\"], color=\"skyblue\", alpha=0.7)\n",
    "        ax[0].set_xlabel(\"Valeurs\")\n",
    "        ax[0].set_ylabel(\"Effectif\")\n",
    "        ax[0].set_title(\"Répartition des valeurs ordinales\")\n",
    "        ax[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # Courbe des effectifs cumulés\n",
    "        ax[1].plot(df_stats[\"Valeur\"], df_stats[\"Effectif cumulé\"], marker=\"o\", linestyle=\"-\", color=\"orange\")\n",
    "        ax[1].set_xlabel(\"Valeurs\")\n",
    "        ax[1].set_ylabel(\"Effectif cumulé\")\n",
    "        ax[1].set_title(\"Effectifs cumulés\")\n",
    "\n",
    "        # Boxplot\n",
    "        if custom_order:\n",
    "            mapping = {val: i for i, val in enumerate(custom_order)}  # Associer chaque valeur à un nombre\n",
    "            series_numeric = series.map(mapping).dropna()  # Appliquer la conversion\n",
    "            ax[2].boxplot(series_numeric, vert=False, patch_artist=True, boxprops=dict(facecolor=\"lightblue\"))\n",
    "            ax[2].set_title(\"Boxplot (Valeurs ordinales converties)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_qualitative_nominal(series: pd.Series, include_graph: bool = False) -> None:\n",
    "    # Initialiser un compteur pour stocker les fréquences des genres\n",
    "    compteur_genres = Counter()\n",
    "\n",
    "    # Parcourir les lignes et compter les occurrences des genres\n",
    "    for genres in series.dropna():\n",
    "        liste_genres = [genre.strip() for genre in genres.split(\",\")]\n",
    "        compteur_genres.update(liste_genres)\n",
    "\n",
    "    # Trier les genres par fréquence décroissante\n",
    "    genres_tries = dict(sorted(compteur_genres.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # Calcul des effectifs, effectifs cumulés, fréquences et fréquences cumulées\n",
    "    total = sum(genres_tries.values())\n",
    "    effectifs_cumules = []\n",
    "    frequences = []\n",
    "    frequences_cumulees = []\n",
    "\n",
    "    cumul = 0\n",
    "    for genre, effectif in genres_tries.items():\n",
    "        cumul += effectif\n",
    "        effectifs_cumules.append(cumul)\n",
    "        frequence = effectif / total\n",
    "        frequences.append(frequence)\n",
    "        frequences_cumulees.append(cumul / total)\n",
    "\n",
    "    # Détermination du mode (les valeurs les plus fréquentes)\n",
    "    max_effectif = max(genres_tries.values())\n",
    "    mode_genres = [genre for genre, effectif in genres_tries.items() if effectif == max_effectif]\n",
    "\n",
    "    # Création du DataFrame pour affichage\n",
    "    df_stats = pd.DataFrame({\n",
    "        \"Genre\": list(genres_tries.keys()),\n",
    "        \"Effectif\": list(genres_tries.values()),\n",
    "        \"Effectif cumulé\": effectifs_cumules,\n",
    "        \"Fréquence\": frequences,\n",
    "        \"Fréquence cumulée\": frequences_cumulees\n",
    "    })\n",
    "\n",
    "    # Affichage du tableau des statistiques\n",
    "    print(\"\\nAnalyse des genres de jeux vidéo :\")\n",
    "    print(df_stats.to_string(index=False, formatters={\"Fréquence\": \"{:.2%}\".format, \"Fréquence cumulée\": \"{:.2%}\".format}))\n",
    "    \n",
    "    # Affichage des statistiques supplémentaires\n",
    "    print(\"\\nMode(s) des genres les plus fréquents :\", \", \".join(mode_genres))\n",
    "\n",
    "    # Affichage des graphiques si demandé\n",
    "    if include_graph:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Histogramme (Bar Chart)\n",
    "        ax[0].bar(df_stats[\"Genre\"], df_stats[\"Effectif\"], color=\"skyblue\", alpha=0.7)\n",
    "        ax[0].set_xlabel(\"Genres\")\n",
    "        ax[0].set_ylabel(\"Effectif\")\n",
    "        ax[0].set_title(\"Répartition des genres\")\n",
    "        ax[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # Diagramme en camembert (Pie Chart)\n",
    "        ax[1].pie(df_stats[\"Effectif\"], labels=df_stats[\"Genre\"], autopct=\"%1.1f%%\", colors=plt.cm.Paired.colors)\n",
    "        ax[1].set_title(\"Répartition des genres (Pie Chart)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_qualitative_nominal(video_games[Variable.genres], include_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_quantitative(video_games[Variable.overall_review_percentage], cutted=False, include_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_qualitative_ordinal(video_games[Variable.overall_review], include_graph=True, custom_order=Variable.overall_review_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_quantitative(video_games[Variable.awards], cutted=False, include_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problématiques bivariées\n",
    "- 1. Y a-t-il une relation entre le prix initial (original_price) et le pourcentage de réduction (discount_percentage) des jeux ?\n",
    "- 2. Le pourcentage de critiques positives (overall_review_%) varie-t-il selon la catégorie du jeu (categories) ?\n",
    "- 3. Les jeux les mieux notés sont-ils vendus plus chers, même après réduction ?\n",
    "- 4. Dans le contexte de la distribution numérique de jeux vidéo, les réductions appliquées sur les titres sont un levier marketing majeur pour attirer les consommateurs. Ces remises sont particulièrement visibles lors d’événements promotionnels majeurs comme les soldes Steam, les festivals de jeux indépendants ou les ventes flash. Cependant, une question se pose : les jeux vidéo vendus à un prix plus élevé bénéficient-ils d’un pourcentage de réduction plus important ?\n",
    "- 5. Comment les caractéristiques d’un jeu vidéo, telles que le prix original, le pourcentage de réduction, le nombre de critiques, les récompenses et la disponibilité de DLC, influencent-elles le pourcentage global des critiques positives (overall_review_%) sur une plateforme comme Steam ? L’objectif est d’identifier les facteurs clés qui contribuent à la réception critique d’un jeu.\n",
    "\n",
    "## Réponses partielles\n",
    "- 1. Une analyse préliminaire pourrait consister à calculer le coefficient de corrélation de Pearson entre ces deux variables quantitatives pour évaluer la force et la direction de leur relation (positive, négative ou nulle). On pourrait aussi visualiser cette relation avec un nuage de points (scatter plot).\n",
    "- 2. On pourrait comparer la moyenne du pourcentage de critiques positives pour chaque catégorie (ex. Solo, Multiplayer) à l’aide d’un test statistique comme une ANOVA ou un test de Kruskal-Wallis si les données ne sont pas normales. Un boxplot pourrait illustrer les différences.\n",
    "- 3. L’intuition sous-jacente est que la qualité perçue d’un jeu influe sur sa capacité à conserver un prix élevé malgré les remises. Un éditeur peut se permettre de moins baisser le prix d’un jeu plébiscité par les joueurs.\n",
    "- 4. Cette interrogation soulève une problématique plus large, qui concerne la stratégie commerciale des éditeurs. Ces derniers peuvent, en théorie, appliquer des réductions plus importantes sur les jeux chers afin de compenser un prix de départ dissuasif. À l’inverse, ils pourraient maintenir une réduction faible pour ne pas dévaluer la valeur perçue du produit. \n",
    "- 5. Nous supposons que des variables comme un prix original élevé, un pourcentage de réduction important, un grand nombre de critiques, des récompenses et la présence de DLC ont un impact significatif sur le pourcentage de critiques positives. Plus précisément, nous pensons que les jeux avec plus de critiques et de récompenses auront un meilleur pourcentage de critiques positives, tandis qu’un prix élevé pourrait avoir un effet négatif.\n",
    "## Problématiques gardées\n",
    "- 3. Les jeux les mieux notés sont-ils vendus plus chers, même après réduction ?\n",
    "- 4. Dans le contexte de la distribution numérique de jeux vidéo, les réductions appliquées sur les titres sont un levier marketing majeur pour attirer les consommateurs. Ces remises sont particulièrement visibles lors d’événements promotionnels majeurs comme les soldes Steam, les festivals de jeux indépendants ou les ventes flash. Cependant, une question se pose : les jeux vidéo vendus à un prix plus élevé bénéficient-ils d’un pourcentage de réduction plus important ?\n",
    "- 5. Comment les caractéristiques d’un jeu vidéo, telles que le prix original, le pourcentage de réduction, le nombre de critiques, les récompenses et la disponibilité de DLC, influencent-elles le pourcentage global des critiques positives (overall_review_%) sur une plateforme comme Steam ? L’objectif est d’identifier les facteurs clés qui contribuent à la réception critique d’un jeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des prix et pourcentages en valeurs numériques\n",
    "def clean_price(price_str):\n",
    "    if pd.isna(price_str) or price_str == 'Free' or price_str == '':\n",
    "        return 0\n",
    "    # Extraction du prix numérique (sans devise)\n",
    "    return float(''.join(char for char in price_str if char.isdigit() or char == '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_discount(discount_str):\n",
    "    if pd.isna(discount_str) or discount_str == '':\n",
    "        return 0\n",
    "    # Extraction du pourcentage (sans le signe %)\n",
    "    return float(discount_str.strip('%').strip('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_problem_1(original_price_series: pd.Series, discount_percentage_series: pd.Series, include_graph: bool = False) -> None:\n",
    "        # Chargement des données\n",
    "    # La première ligne est déjà définie:\n",
    "    # video_games = pd.read_csv(\"data/Jeux_Videos.csv\", sep=\",\")\n",
    "\n",
    "    clean_price_series = original_price_series.apply(clean_price)\n",
    "    clean_discount_series = discount_percentage_series.apply(clean_discount)\n",
    "    \n",
    "    # Création d'un DataFrame temporaire pour l'analyse\n",
    "    temp_df = pd.DataFrame({\n",
    "        'original_price_clean': clean_price_series,\n",
    "        'discount_percentage_clean': clean_discount_series\n",
    "    })\n",
    "    \n",
    "    # Filtrage des jeux payants avec des données valides\n",
    "    analysis_df = temp_df[temp_df['original_price_clean'] > 0].copy()\n",
    "    \n",
    "    print(\"============ ANALYSE DE LA RELATION PRIX INITIAL - RÉDUCTION ============\")\n",
    "    \n",
    "    # 1. Statistiques descriptives\n",
    "    print(\"\\n1. Statistiques descriptives:\")\n",
    "    print(analysis_df.describe())\n",
    "    \n",
    "    # Vérification si les données sont suffisantes pour l'analyse\n",
    "    if len(analysis_df) < 2:\n",
    "        print(\"Pas assez de données pour effectuer l'analyse de corrélation.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Analyse de corrélation\n",
    "    correlation = analysis_df['original_price_clean'].corr(analysis_df['discount_percentage_clean'])\n",
    "    print(f\"\\n2. Coefficient de corrélation entre prix initial et réduction: {correlation:.4f}\")\n",
    "    \n",
    "    # Test de signification statistique (p-value)\n",
    "    corr_test = stats.pearsonr(analysis_df['original_price_clean'], analysis_df['discount_percentage_clean'])\n",
    "    print(f\"   p-value: {corr_test[1]:.4f}\")\n",
    "    \n",
    "    # Interprétation du coefficient de corrélation\n",
    "    print(\"\\n   Interprétation du coefficient de corrélation:\")\n",
    "    if abs(correlation) < 0.1:\n",
    "        strength = \"très faible voire inexistante\"\n",
    "    elif abs(correlation) < 0.3:\n",
    "        strength = \"faible\"\n",
    "    elif abs(correlation) < 0.5:\n",
    "        strength = \"modérée\"\n",
    "    elif abs(correlation) < 0.7:\n",
    "        strength = \"forte\"\n",
    "    else:\n",
    "        strength = \"très forte\"\n",
    "        \n",
    "    direction = \"positive\" if correlation > 0 else \"négative\"\n",
    "    significance = \"statistiquement significative\" if corr_test[1] < 0.05 else \"non statistiquement significative\"\n",
    "    \n",
    "    print(f\"   La corrélation est {strength} et {direction} ({significance}).\")\n",
    "    \n",
    "    # 3. Modélisation linéaire\n",
    "    X = analysis_df[['original_price_clean']]\n",
    "    y = analysis_df['discount_percentage_clean']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    print(f\"\\n3. Modèle linéaire:\")\n",
    "    print(f\"   Équation: discount_percentage = {model.coef_[0]:.4f} × original_price + {model.intercept_:.4f}\")\n",
    "    print(f\"   R² (coefficient de détermination): {model.score(X, y):.4f}\")\n",
    "    \n",
    "    # Interprétation du modèle\n",
    "    print(\"\\n   Interprétation du modèle:\")\n",
    "    if model.coef_[0] > 0:\n",
    "        print(f\"   Pour chaque augmentation de 1 unité du prix initial, le pourcentage de réduction augmente en moyenne de {model.coef_[0]:.4f} points.\")\n",
    "    else:\n",
    "        print(f\"   Pour chaque augmentation de 1 unité du prix initial, le pourcentage de réduction diminue en moyenne de {abs(model.coef_[0]):.4f} points.\")\n",
    "    \n",
    "    # 4. Analyse par tranches de prix\n",
    "    analysis_df['price_range'] = pd.cut(analysis_df['original_price_clean'], \n",
    "                                        bins=[0, 10, 20, 30, 40, 50, float('inf')],\n",
    "                                        labels=['0-10', '10-20', '20-30', '30-40', '40-50', '50+'])\n",
    "    \n",
    "    price_range_analysis = analysis_df.groupby('price_range')['discount_percentage_clean'].agg(['mean', 'count'])\n",
    "    print(\"\\n4. Réduction moyenne par tranche de prix:\")\n",
    "    print(price_range_analysis)\n",
    "    \n",
    "    # 5. Visualisations (conditionnelles)\n",
    "    if include_graph:\n",
    "        # Nuage de points avec ligne de tendance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='original_price_clean', y='discount_percentage_clean', data=analysis_df, alpha=0.6)\n",
    "        sns.regplot(x='original_price_clean', y='discount_percentage_clean', data=analysis_df, \n",
    "                    scatter=False, color='red', line_kws={\"linestyle\": \"--\"})\n",
    "        plt.title('Relation entre prix initial et pourcentage de réduction')\n",
    "        plt.xlabel('Prix initial')\n",
    "        plt.ylabel('Pourcentage de réduction')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Graphique à barres par tranche de prix\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=price_range_analysis.index, y='mean', data=price_range_analysis.reset_index())\n",
    "        plt.title('Pourcentage de réduction moyen par tranche de prix')\n",
    "        plt.xlabel('Tranche de prix')\n",
    "        plt.ylabel('Réduction moyenne (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "        # 6. Conclusion\n",
    "        print(\"\\n5. Conclusion:\")\n",
    "        if abs(correlation) < 0.1:\n",
    "            print(\"   Il n'existe pas de relation linéaire claire entre le prix initial et le pourcentage de réduction.\")\n",
    "        else:\n",
    "            print(f\"   Il existe une relation {strength} {direction} entre le prix initial et le pourcentage de réduction.\")\n",
    "            \n",
    "        if model.score(X, y) < 0.3: \n",
    "            print(\"   Le modèle linéaire explique très peu la variance des réductions, suggérant que d'autres facteurs influencent davantage les politiques de réduction.\")\n",
    "        \n",
    "        print(\"\\n============================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_analysis_problem_1(Variable.original_price, Variable.discount_percentage, include_graph = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
